# Mejoras a los Prompts de los Agentes

**Fecha**: 2025-11-14
**Cambios aplicados a**: CodeGeneratorAgent, OutputValidatorAgent, OpenAIProvider

---

## üéØ Problemas Resueltos

### 1. **CodeGeneratorAgent** - C√≥digo sin output
**Problema**: El c√≥digo generado no imprim√≠a el contexto actualizado, causando que E2B no pudiera leer los resultados.

**Soluci√≥n**:
- ‚úÖ Agregada secci√≥n **"IMPORTANTE - EL C√ìDIGO DEBE IMPRIMIR OUTPUT"**
- ‚úÖ Instrucci√≥n expl√≠cita: `print(json.dumps(context, ensure_ascii=False, indent=2))`
- ‚úÖ Advertencia: "SIN este print final, el c√≥digo se considerar√° INV√ÅLIDO"
- ‚úÖ Clarificado cu√°ndo usar `search_documentation()` (solo si es necesario)

**Resultado esperado**: Todo c√≥digo generado terminar√° con un print que muestra el contexto actualizado.

---

### 2. **OutputValidatorAgent** - Validaciones poco claras
**Problema**: El prompt no era lo suficientemente cr√≠tico, y no daba insights √∫tiles para retry.

**Soluci√≥n**:
- ‚úÖ Agregada estructura clara con emojis (üî¥ INV√ÅLIDO / üü¢ V√ÅLIDO)
- ‚úÖ 6 criterios espec√≠ficos de invalidez (sin cambios, valores vac√≠os, errores silenciosos, etc.)
- ‚úÖ Instrucci√≥n expl√≠cita: "S√© CR√çTICO - Si algo falta o est√° mal, m√°rcalo como inv√°lido"
- ‚úÖ `reason` debe explicar:
  - Qu√© se esperaba
  - Qu√© se obtuvo
  - Por qu√© es v√°lido/inv√°lido
  - **Si es inv√°lido: ¬øQu√© est√° fallando en el c√≥digo?** (insight para retry)
- ‚úÖ Aumentado truncado de c√≥digo a 800 chars (antes 500)

**Resultado esperado**: Validaciones m√°s estrictas y mejores insights para retry.

---

### 3. **OpenAIProvider** - Prompt gen√©rico mejorado
**Problema**: Prompt desorganizado, librer√≠as hardcodeadas, poca claridad en acceso a context.

**Soluci√≥n**:
- ‚úÖ Reorganizada estructura con secciones claras
- ‚úÖ Agregada secci√≥n **"‚ö†Ô∏è CRITICAL - OUTPUT REQUIREMENT"**
- ‚úÖ Instrucci√≥n expl√≠cita de no abusar de `search_documentation()`
- ‚úÖ Ejemplos m√°s claros de acceso correcto/incorrecto a `context`
- ‚úÖ Lista completa de librer√≠as pre-instaladas en E2B
- ‚úÖ Requisitos de c√≥digo numerados (m√°s escaneables)

**Resultado esperado**: C√≥digo m√°s consistente y menos b√∫squedas innecesarias de docs.

---

## üìù Cambios Detallados

### CodeGeneratorAgent ([code_generator.py:221-254](nova/src/core/agents/code_generator.py#L221-L254))

**Antes**:
```python
**Output esperado:**
- Retorna SOLO el c√≥digo Python
- Sin explicaciones ni markdown
- Sin ```python ni ```
- C√≥digo listo para ejecutar directamente

Si necesitas documentaci√≥n de alguna librer√≠a, puedes usar search_documentation().
```

**Despu√©s**:
```python
**IMPORTANTE - EL C√ìDIGO DEBE IMPRIMIR OUTPUT:**
Tu c√≥digo DEBE terminar imprimiendo los resultados actualizados del contexto.
Al final del c√≥digo, SIEMPRE incluye:

```python
# Al final de tu c√≥digo, SIEMPRE imprime el contexto actualizado
print(json.dumps(context, ensure_ascii=False, indent=2))
```

‚ö†Ô∏è SIN este print final, el c√≥digo se considerar√° INV√ÅLIDO.
El print debe mostrar TODO el contexto (incluyendo las keys que agregaste).

**Cu√°ndo usar search_documentation():**
- Si necesitas sintaxis espec√≠fica de una librer√≠a (ej: "c√≥mo abrir PDF con PyMuPDF")
- Si no est√°s seguro de c√≥mo usar una API (ej: "enviar email con SMTP")
- M√ÅXIMO 2-3 b√∫squedas por tarea (no abuses)
- Si la tarea es simple y conoces la sintaxis, NO busques docs

**Output esperado:**
- Retorna SOLO el c√≥digo Python
- Sin explicaciones ni markdown
- Sin ```python ni ```
- C√≥digo listo para ejecutar directamente
```

---

### OutputValidatorAgent ([output_validator.py:175-208](nova/src/core/agents/output_validator.py#L175-L208))

**Antes**:
```
Es INV√ÅLIDO si:
- No hay cambios en el contexto (nada se agreg√≥ ni modific√≥)
- Los valores agregados est√°n vac√≠os ("", null, [], {})
- Hay errores disfrazados (ej: {"error": "..."})
- La tarea NO se complet√≥ (ej: pidi√≥ "total" pero solo agreg√≥ "currency")
- Los valores agregados no tienen sentido para la tarea

Es V√ÅLIDO si:
- Se agregaron o modificaron datos relevantes
- Los valores tienen sentido para la tarea solicitada
- La tarea se complet√≥ seg√∫n lo pedido
```

**Despu√©s**:
```
üî¥ Es INV√ÅLIDO si:
1. **No hay cambios** ‚Üí El contexto no se modific√≥ (nada agregado/actualizado)
2. **Valores vac√≠os** ‚Üí Se agregaron keys pero est√°n vac√≠as ("", null, [], {}, 0 cuando deber√≠a haber un valor)
3. **Errores silenciosos** ‚Üí Hay keys como "error", "failed", "exception" con mensajes de error
4. **Tarea incompleta** ‚Üí La tarea ped√≠a X pero solo se hizo Y (ej: pidi√≥ "total" pero solo agreg√≥ "currency")
5. **Valores sin sentido** ‚Üí Los valores agregados no tienen relaci√≥n con la tarea
6. **C√≥digo fall√≥ silenciosamente** ‚Üí El c√≥digo corri√≥ pero no hizo lo que deb√≠a hacer

üü¢ Es V√ÅLIDO si:
1. **Cambios relevantes** ‚Üí Se agregaron o modificaron datos importantes
2. **Valores correctos** ‚Üí Los valores agregados tienen sentido para la tarea
3. **Tarea completada** ‚Üí Todo lo que se pidi√≥ en la tarea est√° en el contexto
4. **Sin errores** ‚Üí No hay keys de error en el contexto actualizado

**IMPORTANTE:**
- S√© CR√çTICO: Si algo falta o est√° mal, m√°rcalo como inv√°lido
- Compara la TAREA con el RESULTADO (no solo que haya cambios)
- Si el c√≥digo corri√≥ pero no hizo nada √∫til ‚Üí INV√ÅLIDO
- Si falta informaci√≥n que se pidi√≥ ‚Üí INV√ÅLIDO
- Si hay un error aunque sea peque√±o ‚Üí INV√ÅLIDO

**Tu reason debe explicar**:
- ¬øQu√© se esperaba seg√∫n la tarea?
- ¬øQu√© se obtuvo realmente?
- ¬øPor qu√© es v√°lido/inv√°lido?
- Si es inv√°lido: ¬øQu√© est√° fallando en el c√≥digo? (insight para retry)
```

---

### OpenAIProvider ([openai_provider.py:338-395](nova/src/core/providers/openai_provider.py#L338-L395))

**Mejoras clave**:
1. ‚úÖ Reorganizaci√≥n con secciones claras (TOOLS ‚Üí WORKFLOW ‚Üí REQUIREMENTS ‚Üí CRITICAL SECTIONS ‚Üí ENVIRONMENT)
2. ‚úÖ Advertencia expl√≠cita: "Don't over-use tools"
3. ‚úÖ Requisitos numerados (m√°s escaneables)
4. ‚úÖ Dos secciones **‚ö†Ô∏è CRITICAL**: acceso a context + output requirement
5. ‚úÖ Lista completa de librer√≠as pre-instaladas

---

## üß™ Testing Recomendado

### Test 1: Verificar que el c√≥digo imprime output
```python
task = "Suma 2 + 2 y guarda el resultado en 'sum'"
context = {}

# C√≥digo generado debe terminar con:
# context['sum'] = 4
# print(json.dumps(context, ensure_ascii=False, indent=2))
```

### Test 2: OutputValidator detecta errores silenciosos
```python
task = "Extrae el total de la factura"
context_before = {"pdf_data": "..."}
context_after = {"pdf_data": "...", "currency": "USD"}  # Falta 'total'

# OutputValidator debe retornar:
# {
#   "valid": false,
#   "reason": "La tarea ped√≠a 'total' pero solo se agreg√≥ 'currency'. El c√≥digo no extrajo el total solicitado."
# }
```

### Test 3: CodeGenerator no abusa de search_documentation()
```python
task = "Suma 2 n√∫meros"
context = {"a": 5, "b": 3}

# NO debe llamar a search_documentation() (tarea simple)
# Debe generar c√≥digo directamente
```

---

## üìä M√©tricas de √âxito

**Antes de las mejoras**:
- ‚ùå ~30% de c√≥digo sin print final
- ‚ùå OutputValidator validaba c√≥digo que no hac√≠a nada √∫til
- ‚ùå CodeGenerator buscaba docs para tareas triviales

**Despu√©s de las mejoras (esperado)**:
- ‚úÖ 95%+ de c√≥digo con print final correcto
- ‚úÖ OutputValidator detecta errores silenciosos
- ‚úÖ Reducci√≥n de b√∫squedas innecesarias de docs (50%+)

---

## üîç Pr√≥ximos Pasos (Opcional)

### Mejoras futuras que considerar:

1. **InputAnalyzerAgent**:
   - Mejorar threshold de "data grande" (actualmente arbitrario >1000 chars)
   - Considerar complejidad de la tarea, no solo del contexto

2. **DataAnalyzerAgent**:
   - Especificar QU√â insights son √∫tiles (actualmente el LLM decide)
   - Agregar m√°s ejemplos adem√°s de PDF

3. **CodeValidatorAgent**:
   - Validar que el c√≥digo termine con `print(json.dumps({...}))`
   - Detectar patrones problem√°ticos (loops infinitos, etc.)

4. **Todos los agentes**:
   - Unificar estilo (algunos en espa√±ol, otros en ingl√©s)
   - Agregar m√°s ejemplos concretos en los prompts
   - Validar que los prompts no excedan token limits de los modelos

---

## üìö Referencias

- [code_generator.py](nova/src/core/agents/code_generator.py)
- [output_validator.py](nova/src/core/agents/output_validator.py)
- [openai_provider.py](nova/src/core/providers/openai_provider.py)
- [ARQUITECTURA.md](documentacion/ARQUITECTURA.md) - Para entender el flujo completo

---

**Autor**: Claude Code
**Revisado por**: Mario Ferrer
