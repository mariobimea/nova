"""
Code Cache Manager
Manages code cache storage, retrieval, and statistics
"""

import logging
from typing import Dict, Any, Optional
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import func

from ..models.code_cache import CodeCache
from .cache_utils import (
    generate_cache_key,
    generate_task_hash,
    generate_context_schema_hash
)

logger = logging.getLogger(__name__)


class CodeCacheManager:
    """
    Manages AI-generated code cache.

    Responsibilities:
    - Generate cache keys from prompt + context
    - Lookup cached code
    - Save new code to cache
    - Track usage statistics (hits, misses, success/failure)
    - Provide cache performance metrics

    Usage:
        cache = CodeCacheManager(db_session)

        # Try to get cached code
        cached = await cache.lookup(prompt, context)

        if cached:
            # Cache HIT
            code = cached.generated_code
            await cache.record_success(cached.cache_key, execution_time_ms)
        else:
            # Cache MISS
            code = await ai.generate_code(prompt, context)
            await cache.save(prompt, context, code, model, tokens, cost)
    """

    def __init__(self, db: Session):
        """
        Initialize cache manager.

        Args:
            db: SQLAlchemy database session
        """
        self.db = db

    async def lookup(
        self,
        prompt: str,
        context: Dict[str, Any]
    ) -> Optional[CodeCache]:
        """
        Lookup code in cache.

        Args:
            prompt: Task description
            context: Complete execution context

        Returns:
            CodeCache entry if found, None otherwise
        """
        # Generate cache key
        cache_key = generate_cache_key(prompt, context)

        # Query database
        entry = self.db.query(CodeCache).filter(
            CodeCache.cache_key == cache_key
        ).first()

        if entry:
            logger.info(f"üéØ Cache HIT: {cache_key[:16]}... (reused {entry.times_reused} times, {entry.success_rate:.1%} success rate)")

            # Update last_used_at
            entry.last_used_at = datetime.utcnow()
            self.db.commit()

            return entry
        else:
            logger.info(f"‚ùå Cache MISS: {cache_key[:16]}...")
            return None

    async def save(
        self,
        prompt: str,
        context: Dict[str, Any],
        generated_code: str,
        model: str,
        tokens_used: int,
        cost_usd: float,
        workflow_id: Optional[int] = None,
        node_id: Optional[str] = None
    ) -> CodeCache:
        """
        Save generated code to cache.

        Args:
            prompt: Original task description
            context: Complete execution context
            generated_code: Python code generated by AI
            model: AI model used (e.g., "gpt-4o-mini")
            tokens_used: Total tokens consumed
            cost_usd: Cost in USD
            workflow_id: Optional workflow ID
            node_id: Optional node ID

        Returns:
            Created CodeCache entry
        """
        # Generate hashes
        cache_key = generate_cache_key(prompt, context)
        task_hash = generate_task_hash(prompt)
        context_schema_hash = generate_context_schema_hash(context)

        # Check if already exists (idempotent)
        existing = self.db.query(CodeCache).filter(
            CodeCache.cache_key == cache_key
        ).first()

        if existing:
            logger.debug(f"Cache entry already exists: {cache_key[:16]}...")
            return existing

        # Create new entry
        entry = CodeCache(
            cache_key=cache_key,
            task_hash=task_hash,
            context_schema_hash=context_schema_hash,
            generated_code=generated_code,
            model=model,
            original_prompt=prompt,
            tokens_used=tokens_used,
            cost_usd=cost_usd,
            times_reused=0,
            success_count=1,  # First execution was successful
            failure_count=0,
            workflow_id=workflow_id,
            node_id=node_id
        )

        self.db.add(entry)
        self.db.commit()
        self.db.refresh(entry)

        logger.info(
            f"üíæ Saved to cache: {cache_key[:16]}... "
            f"(cost: ${cost_usd:.4f}, tokens: {tokens_used})"
        )

        return entry

    async def record_success(
        self,
        cache_key: str,
        execution_time_ms: float
    ):
        """
        Record successful execution of cached code.

        Updates:
        - times_reused += 1
        - success_count += 1
        - avg_execution_time_ms (rolling average)
        - last_used_at

        Args:
            cache_key: Cache key
            execution_time_ms: Execution time in milliseconds
        """
        entry = self.db.query(CodeCache).filter(
            CodeCache.cache_key == cache_key
        ).first()

        if not entry:
            logger.warning(f"Cache entry not found: {cache_key[:16]}...")
            return

        # Update statistics
        entry.times_reused += 1
        entry.success_count += 1
        entry.last_used_at = datetime.utcnow()

        # Update rolling average execution time
        if entry.avg_execution_time_ms is None:
            entry.avg_execution_time_ms = execution_time_ms
        else:
            # Weighted average (80% old, 20% new)
            entry.avg_execution_time_ms = (
                entry.avg_execution_time_ms * 0.8 +
                execution_time_ms * 0.2
            )

        self.db.commit()

        logger.debug(
            f"‚úÖ Cache success recorded: {cache_key[:16]}... "
            f"(total reuses: {entry.times_reused}, success rate: {entry.success_rate:.1%})"
        )

    async def record_failure(
        self,
        cache_key: str,
        error_message: Optional[str] = None
    ):
        """
        Record failed execution of cached code.

        Updates:
        - times_reused += 1 (attempted to use)
        - failure_count += 1
        - last_used_at

        If failure rate exceeds 30%, the cache entry is considered unreliable
        and should be deleted/regenerated.

        Args:
            cache_key: Cache key
            error_message: Optional error message for logging
        """
        entry = self.db.query(CodeCache).filter(
            CodeCache.cache_key == cache_key
        ).first()

        if not entry:
            logger.warning(f"Cache entry not found: {cache_key[:16]}...")
            return

        # Update statistics
        entry.times_reused += 1
        entry.failure_count += 1
        entry.last_used_at = datetime.utcnow()

        self.db.commit()

        # Log warning
        logger.warning(
            f"‚ùå Cache failure recorded: {cache_key[:16]}... "
            f"(failures: {entry.failure_count}/{entry.times_reused}, "
            f"success rate: {entry.success_rate:.1%})"
        )

        if error_message:
            logger.warning(f"Error: {error_message}")

        # Check if entry is unreliable
        if entry.success_rate < 0.7 and entry.times_reused >= 3:
            logger.error(
                f"üóëÔ∏è  Cache entry unreliable (success rate: {entry.success_rate:.1%}). "
                f"Consider invalidating: {cache_key[:16]}..."
            )

    async def delete(self, cache_key: str):
        """
        Delete cache entry (invalidation).

        Use this when:
        - Cached code is consistently failing
        - Code is outdated (API changes, etc.)
        - Manual cache invalidation requested

        Args:
            cache_key: Cache key to delete
        """
        entry = self.db.query(CodeCache).filter(
            CodeCache.cache_key == cache_key
        ).first()

        if entry:
            self.db.delete(entry)
            self.db.commit()
            logger.info(f"üóëÔ∏è  Deleted cache entry: {cache_key[:16]}...")
        else:
            logger.warning(f"Cache entry not found: {cache_key[:16]}...")

    async def get_stats(self) -> Dict[str, Any]:
        """
        Get cache performance statistics.

        Returns:
            Dictionary with cache metrics:
            - total_cached_codes: Number of unique codes cached
            - total_cache_hits: Total number of cache reuses
            - total_cost_saved_usd: Total cost saved by reusing code
            - avg_reuse_per_code: Average times each code is reused
            - success_rate: Overall success rate across all cache entries
            - top_codes: Top 10 most reused codes
        """
        # Total entries
        total_entries = self.db.query(CodeCache).count()

        # Total cache hits (reuses)
        total_reuses = self.db.query(
            func.sum(CodeCache.times_reused)
        ).scalar() or 0

        # Total cost saved
        total_savings = self.db.query(
            func.sum(CodeCache.cost_usd * CodeCache.times_reused)
        ).scalar() or 0.0

        # Success/failure counts
        total_successes = self.db.query(
            func.sum(CodeCache.success_count)
        ).scalar() or 0

        total_failures = self.db.query(
            func.sum(CodeCache.failure_count)
        ).scalar() or 0

        total_executions = total_successes + total_failures
        overall_success_rate = (
            total_successes / total_executions if total_executions > 0 else 0.0
        )

        # Top 10 most reused codes
        top_codes = self.db.query(CodeCache).order_by(
            CodeCache.times_reused.desc()
        ).limit(10).all()

        return {
            "total_cached_codes": total_entries,
            "total_cache_hits": int(total_reuses),
            "total_cost_saved_usd": float(total_savings),
            "avg_reuse_per_code": total_reuses / max(total_entries, 1),
            "overall_success_rate": overall_success_rate,
            "total_successes": int(total_successes),
            "total_failures": int(total_failures),
            "top_codes": [
                {
                    "cache_key": code.cache_key[:16] + "...",
                    "times_reused": code.times_reused,
                    "success_rate": code.success_rate,
                    "cost_saved_usd": code.total_cost_saved_usd,
                    "node_id": code.node_id,
                    "workflow_id": code.workflow_id
                }
                for code in top_codes
            ]
        }

    async def cleanup_old_entries(self, days_old: int = 90):
        """
        Clean up old cache entries that haven't been used recently.

        Args:
            days_old: Delete entries not used in this many days
        """
        from datetime import timedelta

        cutoff_date = datetime.utcnow() - timedelta(days=days_old)

        deleted = self.db.query(CodeCache).filter(
            CodeCache.last_used_at < cutoff_date
        ).delete()

        self.db.commit()

        logger.info(f"üßπ Cleaned up {deleted} old cache entries (>{days_old} days old)")

        return deleted
