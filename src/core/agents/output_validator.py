"""
OutputValidatorAgent - Valida resultado despuÃ©s de ejecutar.

Responsabilidad:
    Validar el resultado DESPUÃ‰S de ejecutar (validaciÃ³n semÃ¡ntica).

CaracterÃ­sticas:
    - Modelo: gpt-4o-mini (validaciÃ³n simple)
    - Ejecuciones: DespuÃ©s de cada ejecuciÃ³n exitosa en E2B
    - Tool calling: NO
    - Costo: ~$0.0005 por ejecuciÃ³n
"""

from typing import Dict
import json
import time
from openai import AsyncOpenAI

from .base import BaseAgent, AgentResponse


class OutputValidatorAgent(BaseAgent):
    """Valida semÃ¡nticamente si la tarea se completÃ³ correctamente"""

    def __init__(self, openai_client: AsyncOpenAI):
        super().__init__("OutputValidator")
        self.client = openai_client
        self.model = "gpt-4o-mini"

    async def execute(
        self,
        task: str,
        context_before: Dict,
        context_after: Dict,
        generated_code: str = None
    ) -> AgentResponse:
        """
        Valida semÃ¡nticamente si la tarea se completÃ³ correctamente.

        Args:
            task: Tarea que se solicitÃ³ resolver
            context_before: Contexto antes de la ejecuciÃ³n
            context_after: Contexto despuÃ©s de la ejecuciÃ³n
            generated_code: CÃ³digo generado que se ejecutÃ³ (opcional, para debugging)

        Returns:
            AgentResponse con:
                - valid: bool
                - reason: str (por quÃ© es vÃ¡lido o invÃ¡lido)
                - changes_detected: List[str] (keys modificadas/agregadas)
        """
        try:
            start_time = time.time()

            # Detectar cambios
            changes = self._detect_changes(context_before, context_after)

            # Construir prompt
            prompt = self._build_prompt(task, context_before, context_after, changes, generated_code)

            # Llamar a OpenAI
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "Eres un validador que verifica si las tareas se completaron correctamente. Respondes SOLO en JSON."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                response_format={"type": "json_object"},
                timeout=30.0  # 30 segundos timeout
            )

            execution_time_ms = (time.time() - start_time) * 1000

            # Parsear respuesta
            result = json.loads(response.choices[0].message.content)

            # Validar estructura
            required_keys = ["valid", "reason"]
            if not all(k in result for k in required_keys):
                raise ValueError(f"Respuesta invÃ¡lida, faltan keys: {required_keys}")

            # Agregar cambios detectados
            result["changes_detected"] = changes

            # Agregar metadata AI
            usage = response.usage
            tokens_input = usage.prompt_tokens if usage else 0
            tokens_output = usage.completion_tokens if usage else 0
            cost_usd = (tokens_input * 0.150 / 1_000_000) + (tokens_output * 0.600 / 1_000_000)

            result["model"] = self.model
            result["tokens"] = {
                "input": tokens_input,
                "output": tokens_output
            }
            result["cost_usd"] = cost_usd

            if result["valid"]:
                self.logger.info(f"âœ… Output vÃ¡lido: {result['reason']}")
            else:
                self.logger.warning(f"âŒ Output invÃ¡lido: {result['reason']}")

            return self._create_response(
                success=True,
                data=result,
                execution_time_ms=execution_time_ms
            )

        except Exception as e:
            self.logger.error(f"Error en OutputValidator: {str(e)}")
            return self._create_response(
                success=False,
                error=str(e),
                execution_time_ms=0.0
            )

    def _detect_changes(self, before: Dict, after: Dict) -> list:
        """Detecta quÃ© keys cambiaron entre before y after"""
        changes = []

        # Keys agregadas o modificadas
        for key in after.keys():
            if key not in before:
                changes.append(key)
            elif before[key] != after[key]:
                changes.append(key)

        return changes

    def _build_prompt(
        self,
        task: str,
        context_before: Dict,
        context_after: Dict,
        changes: list,
        generated_code: str = None
    ) -> str:
        """Construye el prompt para validaciÃ³n"""

        # Preparar contextos de forma compacta
        before_summary = self._summarize_context(context_before)
        after_summary = self._summarize_context(context_after)

        prompt = f"""Tu trabajo: Validar si la tarea se completÃ³ correctamente despuÃ©s de ejecutar el cÃ³digo.

**Tarea solicitada:** {task}

**Contexto ANTES de ejecutar:**
{json.dumps(before_summary, indent=2)}

**Contexto DESPUÃ‰S de ejecutar:**
{json.dumps(after_summary, indent=2)}

**Cambios detectados:** {changes if changes else "Ninguno"}
"""

        # Agregar cÃ³digo generado si estÃ¡ disponible (para mejor contexto)
        if generated_code:
            # Truncar cÃ³digo si es muy largo (max 800 chars para el prompt)
            code_preview = generated_code[:800] + "..." if len(generated_code) > 800 else generated_code
            prompt += f"""
**CÃ³digo que se ejecutÃ³:**
```python
{code_preview}
```
"""

        prompt += """
Devuelve JSON:
{
  "valid": true/false,
  "reason": "ExplicaciÃ³n detallada de por quÃ© es vÃ¡lido o invÃ¡lido"
}

ðŸ”´ Es INVÃLIDO si:
1. **No hay cambios** â†’ El contexto no se modificÃ³ (nada agregado/actualizado)
2. **Valores vacÃ­os** â†’ Se agregaron keys pero estÃ¡n vacÃ­as ("", null, [], {}, 0 cuando deberÃ­a haber un valor)
3. **Errores silenciosos** â†’ Hay keys como "error", "failed", "exception" con mensajes de error
4. **Tarea incompleta** â†’ La tarea pedÃ­a X pero solo se hizo Y (ej: pidiÃ³ "total" pero solo agregÃ³ "currency")
5. **Valores sin sentido** â†’ Los valores agregados no tienen relaciÃ³n con la tarea
6. **CÃ³digo fallÃ³ silenciosamente** â†’ El cÃ³digo corriÃ³ pero no hizo lo que debÃ­a hacer

ðŸŸ¢ Es VÃLIDO si:
1. **Cambios relevantes** â†’ Se agregaron o modificaron datos importantes
2. **Valores correctos** â†’ Los valores agregados tienen sentido para la tarea
3. **Tarea completada** â†’ Todo lo que se pidiÃ³ en la tarea estÃ¡ en el contexto
4. **Sin errores** â†’ No hay keys de error en el contexto actualizado

**IMPORTANTE:**
- SÃ© CRÃTICO: Si algo falta o estÃ¡ mal, mÃ¡rcalo como invÃ¡lido
- Compara la TAREA con el RESULTADO (no solo que haya cambios)
- Si el cÃ³digo corriÃ³ pero no hizo nada Ãºtil â†’ INVÃLIDO
- Si falta informaciÃ³n que se pidiÃ³ â†’ INVÃLIDO
- Si hay un error aunque sea pequeÃ±o â†’ INVÃLIDO

**Tu reason debe explicar**:
- Â¿QuÃ© se esperaba segÃºn la tarea?
- Â¿QuÃ© se obtuvo realmente?
- Â¿Por quÃ© es vÃ¡lido/invÃ¡lido?
- Si es invÃ¡lido: Â¿QuÃ© estÃ¡ fallando en el cÃ³digo? (insight para retry)
"""
        return prompt

    def _summarize_context(self, context: Dict) -> Dict:
        """Resume el contexto para el prompt (evita enviar data muy grande)"""
        summary = {}

        for key, value in context.items():
            if isinstance(value, str):
                if len(value) > 100:
                    summary[key] = f"<string length={len(value)}>"
                else:
                    summary[key] = value
            elif isinstance(value, (list, dict)):
                summary[key] = f"<{type(value).__name__} with {len(value)} items>"
            else:
                summary[key] = value

        return summary
