"""
AnalysisValidatorAgent - Valida insights del DataAnalyzer.

Responsabilidad:
    Validar que los insights generados sean √∫tiles para resolver la tarea.

Caracter√≠sticas:
    - Modelo: gpt-4o-mini (validaci√≥n r√°pida)
    - Ejecuciones: Despu√©s de cada DataAnalyzer
    - Tool calling: NO
    - Costo: ~$0.0003 por validaci√≥n
"""

from typing import Dict
import json
import time
from openai import AsyncOpenAI

from .base import BaseAgent, AgentResponse


class AnalysisValidatorAgent(BaseAgent):
    """Valida que los insights del DataAnalyzer sean √∫tiles"""

    def __init__(self, openai_client: AsyncOpenAI):
        super().__init__("AnalysisValidator")
        self.client = openai_client
        self.model = "gpt-4o-mini"

    async def execute(
        self,
        task: str,
        insights: Dict,
        context_schema: Dict,
        analysis_code: str = None
    ) -> AgentResponse:
        """
        Valida que los insights sean √∫tiles.

        Args:
            task: Tarea original a resolver
            insights: Insights generados por DataAnalyzer
            context_schema: Schema del contexto original
            analysis_code: C√≥digo de an√°lisis ejecutado (para debugging)

        Returns:
            AgentResponse con:
                - valid: bool
                - reason: str (por qu√© es v√°lido/inv√°lido)
                - suggestions: List[str] (qu√© mejorar)
                - model: str
                - tokens: dict
                - cost_usd: float
        """
        try:
            start_time = time.time()

            # Construir prompt
            prompt = self._build_prompt(task, insights, context_schema, analysis_code)

            # Llamar a OpenAI
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "Eres un validador de an√°lisis de datos. Respondes SOLO en JSON."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                response_format={"type": "json_object"},
                timeout=30.0
            )

            execution_time_ms = (time.time() - start_time) * 1000

            # Parsear respuesta
            result = json.loads(response.choices[0].message.content)

            # Validar estructura
            required_keys = ["valid", "reason"]
            if not all(k in result for k in required_keys):
                raise ValueError(f"Respuesta inv√°lida, faltan keys: {required_keys}")

            # Agregar metadata AI
            usage = response.usage
            tokens_input = usage.prompt_tokens if usage else 0
            tokens_output = usage.completion_tokens if usage else 0
            cost_usd = (tokens_input * 0.150 / 1_000_000) + (tokens_output * 0.600 / 1_000_000)

            result["model"] = self.model
            result["tokens"] = {
                "input": tokens_input,
                "output": tokens_output
            }
            result["cost_usd"] = cost_usd

            if result["valid"]:
                self.logger.info(f"‚úÖ Insights v√°lidos: {result['reason']}")
            else:
                self.logger.warning(f"‚ùå Insights inv√°lidos: {result['reason']}")

            return self._create_response(
                success=True,
                data=result,
                execution_time_ms=execution_time_ms
            )

        except Exception as e:
            self.logger.error(f"Error en AnalysisValidator: {str(e)}")
            return self._create_response(
                success=False,
                error=str(e),
                execution_time_ms=0.0
            )

    def _build_prompt(
        self,
        task: str,
        insights: Dict,
        context_schema: Dict,
        analysis_code: str = None
    ) -> str:
        """Construye el prompt para validaci√≥n"""

        prompt = f"""Valida estos insights de an√°lisis de datos.

**Tarea original:** {task}

**Contexto schema:**
{json.dumps(context_schema, indent=2, ensure_ascii=False)}

**Insights generados:**
{json.dumps(insights, indent=2, ensure_ascii=False)}
"""

        if analysis_code:
            # Mostrar solo primeras/√∫ltimas l√≠neas del c√≥digo
            code_lines = analysis_code.split("\n")
            if len(code_lines) > 20:
                code_preview = "\n".join(code_lines[:10]) + "\n...\n" + "\n".join(code_lines[-5:])
            else:
                code_preview = analysis_code

            prompt += f"""
**C√≥digo de an√°lisis ejecutado:**
```python
{code_preview}
```
"""

        prompt += """
Devuelve JSON:
{
  "valid": true/false,
  "reason": "Explicaci√≥n detallada",
  "suggestions": ["sugerencia 1", "sugerencia 2"]  // solo si invalid
}

üî¥ Los insights son INV√ÅLIDOS si:
1. **type = "unknown"** ‚Üí No detect√≥ el tipo de data real
2. **Falta metadata cr√≠tica** ‚Üí type="pdf" pero sin pages, has_text_layer, etc.
3. **Metadata in√∫til** ‚Üí Solo copia keys del context sin analizar la data real
4. **Error en insights** ‚Üí Tiene key "error" indicando que el an√°lisis fall√≥
5. **No ayuda para la tarea** ‚Üí Los insights no son √∫tiles para resolver la tarea
6. **Valores sin sentido** ‚Üí Metadata que no corresponde al tipo de data

üü¢ Los insights son V√ÅLIDOS si:
1. **type detectado correctamente** ‚Üí type="pdf"/"image"/"email"/etc (NO "unknown")
2. **Metadata √∫til y espec√≠fica** ‚Üí Informaci√≥n estructural relevante (pages, format, size, has_text_layer, etc.)
3. **Relevante para tarea** ‚Üí La metadata ayudar√° al CodeGenerator a resolver la tarea
4. **Sin errores** ‚Üí No hay crashes ni fallos en el an√°lisis
5. **Valores razonables** ‚Üí La metadata tiene sentido (ej: pages > 0 para PDF)

**Ejemplos de insights V√ÅLIDOS:**

Para PDF:
{
  "type": "pdf",
  "pages": 3,
  "has_text_layer": true,
  "filename": "invoice.pdf"
}

Para Imagen:
{
  "type": "image",
  "format": "PNG",
  "size": [1920, 1080],
  "has_text": true
}

Para Email:
{
  "type": "email",
  "has_attachments": true,
  "attachment_count": 2,
  "subject": "Invoice #1234"
}

**Ejemplos de insights INV√ÅLIDOS:**

{
  "type": "unknown"  // ‚ùå No detect√≥ el tipo
}

{
  "type": "pdf"  // ‚ùå Falta metadata (pages, has_text_layer)
}

{
  "type": "pdf",
  "error": "Could not parse"  // ‚ùå Fall√≥ el an√°lisis
}

**Suggestions (solo si invalid):**
- Espec√≠ficas y accionables
- Qu√© metadata deber√≠a extraer
- Qu√© librer√≠as deber√≠a usar
- C√≥mo corregir el error

Ejemplo:
[
  "Usa PyMuPDF para detectar el n√∫mero de p√°ginas: len(doc)",
  "Verifica si tiene capa de texto con: doc[0].get_text()",
  "Extrae el filename de context['attachments'][0]['filename']"
]

**IMPORTANTE:**
- S√© CR√çTICO: Si type="unknown" o falta metadata esencial ‚Üí INV√ÅLIDO
- Compara los insights con el contexto schema para verificar que realmente analiz√≥ la data
- Si los insights ayudar√°n al CodeGenerator ‚Üí V√ÅLIDO
- Si son gen√©ricos o vac√≠os ‚Üí INV√ÅLIDO
"""
        return prompt
