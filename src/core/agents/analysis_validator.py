"""
AnalysisValidatorAgent - Valida insights del DataAnalyzer.

Responsabilidad:
    Validar que los insights generados sean √∫tiles para resolver la tarea.

Caracter√≠sticas:
    - Modelo: gpt-4o-mini (validaci√≥n r√°pida)
    - Ejecuciones: Despu√©s de cada DataAnalyzer
    - Tool calling: NO
    - Costo: ~$0.0003 por validaci√≥n
"""

from typing import Dict
import json
import time
from openai import AsyncOpenAI

from .base import BaseAgent, AgentResponse


class AnalysisValidatorAgent(BaseAgent):
    """Valida que los insights del DataAnalyzer sean √∫tiles"""

    def __init__(self, openai_client: AsyncOpenAI):
        super().__init__("AnalysisValidator")
        self.client = openai_client
        self.model = "gpt-4o-mini"

    async def execute(
        self,
        task: str,
        insights: Dict,
        context_schema: Dict,
        analysis_code: str = None
    ) -> AgentResponse:
        """
        Valida que los insights sean √∫tiles.

        Args:
            task: Tarea original a resolver
            insights: Insights generados por DataAnalyzer
            context_schema: Schema del contexto original
            analysis_code: C√≥digo de an√°lisis ejecutado (para debugging)

        Returns:
            AgentResponse con:
                - valid: bool
                - reason: str (por qu√© es v√°lido/inv√°lido)
                - suggestions: List[str] (qu√© mejorar)
                - model: str
                - tokens: dict
                - cost_usd: float
        """
        try:
            start_time = time.time()

            # Construir prompt
            prompt = self._build_prompt(task, insights, context_schema, analysis_code)

            # Llamar a OpenAI
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "Eres un validador de an√°lisis de datos. Respondes SOLO en JSON."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                response_format={"type": "json_object"},
                timeout=30.0
            )

            execution_time_ms = (time.time() - start_time) * 1000

            # Parsear respuesta
            result = json.loads(response.choices[0].message.content)

            # Validar estructura
            required_keys = ["valid", "reason"]
            if not all(k in result for k in required_keys):
                raise ValueError(f"Respuesta inv√°lida, faltan keys: {required_keys}")

            # Agregar metadata AI
            usage = response.usage
            tokens_input = usage.prompt_tokens if usage else 0
            tokens_output = usage.completion_tokens if usage else 0
            cost_usd = (tokens_input * 0.150 / 1_000_000) + (tokens_output * 0.600 / 1_000_000)

            result["model"] = self.model
            result["tokens"] = {
                "input": tokens_input,
                "output": tokens_output
            }
            result["cost_usd"] = cost_usd

            if result["valid"]:
                self.logger.info(f"‚úÖ Insights v√°lidos: {result['reason']}")
            else:
                self.logger.warning(f"‚ùå Insights inv√°lidos: {result['reason']}")

            return self._create_response(
                success=True,
                data=result,
                execution_time_ms=execution_time_ms
            )

        except Exception as e:
            self.logger.error(f"Error en AnalysisValidator: {str(e)}")
            return self._create_response(
                success=False,
                error=str(e),
                execution_time_ms=0.0
            )

    def _build_prompt(
        self,
        task: str,
        insights: Dict,
        context_schema: Dict,
        analysis_code: str = None
    ) -> str:
        """Construye el prompt para validaci√≥n"""

        prompt = f"""Valida estos insights de an√°lisis de datos.

**Tarea original:** {task}

**Contexto schema:**
{json.dumps(context_schema, indent=2, ensure_ascii=False)}

**Insights generados:**
{json.dumps(insights, indent=2, ensure_ascii=False)}
"""

        if analysis_code:
            # Mostrar solo primeras/√∫ltimas l√≠neas del c√≥digo
            code_lines = analysis_code.split("\n")
            if len(code_lines) > 20:
                code_preview = "\n".join(code_lines[:10]) + "\n...\n" + "\n".join(code_lines[-5:])
            else:
                code_preview = analysis_code

            prompt += f"""
**C√≥digo de an√°lisis ejecutado:**
```python
{code_preview}
```
"""

        prompt += """
**INSTRUCCIONES DE VALIDACI√ìN:**

Analiza los insights generados y determina si son V√ÅLIDOS o INV√ÅLIDOS.

üü¢ **Los insights son V√ÅLIDOS si contienen metadata √∫til:**

Para PDFs, debe tener:
- type: "pdf"
- pages: n√∫mero (ej: 1, 3, 5)
- has_text_layer: true/false
- filename: string

Para Im√°genes, debe tener:
- type: "image"
- format: "PNG"/"JPEG"/etc
- size: [width, height]
- has_text: true/false (si contiene texto visible)

Para Emails, debe tener:
- type: "email"
- has_attachments: true/false
- attachment_count: n√∫mero
- subject: string

üî¥ **Los insights son INV√ÅLIDOS solo si:**
1. type = "unknown" (no detect√≥ el tipo)
2. type est√° definido PERO faltan TODAS las dem√°s keys de metadata
3. Contiene key "error" indicando fallo
4. Metadata vac√≠a o sin sentido

**IMPORTANTE:**
- Si type="pdf" Y tiene "pages" Y tiene "has_text_layer" ‚Üí ES V√ÅLIDO (aunque has_text_layer sea false)
- has_text_layer=false es V√ÅLIDO y √∫til (indica que necesita OCR)
- NO exijas metadata que no est√© en los ejemplos de arriba
- Si los insights contienen la informaci√≥n estructural b√°sica ‚Üí ES V√ÅLIDO

Devuelve JSON:
{
  "valid": true/false,
  "reason": "Explicaci√≥n concreta de por qu√© es v√°lido/inv√°lido",
  "suggestions": ["sugerencia 1", "sugerencia 2"]  // solo si invalid
}
"""
        return prompt
