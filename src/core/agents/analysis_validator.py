"""
AnalysisValidatorAgent - Valida insights del DataAnalyzer.

Responsabilidad:
    Validar que los insights generados sean Ãºtiles para resolver la tarea.

CaracterÃ­sticas:
    - Modelo: gpt-4o-mini (validaciÃ³n rÃ¡pida)
    - Ejecuciones: DespuÃ©s de cada DataAnalyzer
    - Tool calling: NO
    - Costo: ~$0.0003 por validaciÃ³n
"""

from typing import Dict
import json
import time
from openai import AsyncOpenAI

from .base import BaseAgent, AgentResponse


class AnalysisValidatorAgent(BaseAgent):
    """Valida que los insights del DataAnalyzer sean Ãºtiles"""

    def __init__(self, openai_client: AsyncOpenAI):
        super().__init__("AnalysisValidator")
        self.client = openai_client
        self.model = "gpt-4o-mini"

    async def execute(
        self,
        task: str,
        insights: Dict,
        context_schema: Dict,
        analysis_code: str = None
    ) -> AgentResponse:
        """
        Valida que los insights sean Ãºtiles.

        Args:
            task: Tarea original a resolver
            insights: Insights generados por DataAnalyzer
            context_schema: Schema del contexto original
            analysis_code: CÃ³digo de anÃ¡lisis ejecutado (para debugging)

        Returns:
            AgentResponse con:
                - valid: bool
                - reason: str (por quÃ© es vÃ¡lido/invÃ¡lido)
                - suggestions: List[str] (quÃ© mejorar)
                - model: str
                - tokens: dict
                - cost_usd: float
        """
        try:
            start_time = time.time()

            # Construir prompt
            prompt = self._build_prompt(task, insights, context_schema, analysis_code)

            # Llamar a OpenAI
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "Eres un validador de anÃ¡lisis de datos. Respondes SOLO en JSON."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                response_format={"type": "json_object"},
                timeout=30.0
            )

            execution_time_ms = (time.time() - start_time) * 1000

            # Parsear respuesta
            result = json.loads(response.choices[0].message.content)

            # Validar estructura
            required_keys = ["valid", "reason"]
            if not all(k in result for k in required_keys):
                raise ValueError(f"Respuesta invÃ¡lida, faltan keys: {required_keys}")

            # Agregar metadata AI
            usage = response.usage
            tokens_input = usage.prompt_tokens if usage else 0
            tokens_output = usage.completion_tokens if usage else 0
            cost_usd = (tokens_input * 0.150 / 1_000_000) + (tokens_output * 0.600 / 1_000_000)

            result["model"] = self.model
            result["tokens"] = {
                "input": tokens_input,
                "output": tokens_output
            }
            result["cost_usd"] = cost_usd

            if result["valid"]:
                self.logger.info(f"âœ… Insights vÃ¡lidos: {result['reason']}")
            else:
                self.logger.warning(f"âŒ Insights invÃ¡lidos: {result['reason']}")

            return self._create_response(
                success=True,
                data=result,
                execution_time_ms=execution_time_ms
            )

        except Exception as e:
            self.logger.error(f"Error en AnalysisValidator: {str(e)}")
            return self._create_response(
                success=False,
                error=str(e),
                execution_time_ms=0.0
            )

    def _build_prompt(
        self,
        task: str,
        insights: Dict,
        context_schema: Dict,
        analysis_code: str = None
    ) -> str:
        """Construye el prompt para validaciÃ³n"""

        prompt = f"""Tu trabajo: Validar si los insights generados son Ãºtiles para resolver la tarea.

**Tarea original:** {task}

**Contexto schema:**
{json.dumps(context_schema, indent=2, ensure_ascii=False)}

**Insights generados:**
{json.dumps(insights, indent=2, ensure_ascii=False)}
"""

        if analysis_code:
            # Mostrar solo primeras/Ãºltimas lÃ­neas del cÃ³digo
            code_lines = analysis_code.split("\n")
            if len(code_lines) > 20:
                code_preview = "\n".join(code_lines[:10]) + "\n...\n" + "\n".join(code_lines[-5:])
            else:
                code_preview = analysis_code

            prompt += f"""
**CÃ³digo de anÃ¡lisis ejecutado:**
```python
{code_preview}
```
"""

        prompt += """
Devuelve JSON:
{
  "valid": true/false,
  "reason": "ExplicaciÃ³n detallada de por quÃ© es vÃ¡lido o invÃ¡lido",
  "suggestions": ["sugerencia 1", "sugerencia 2"]  // solo si invalid
}

ğŸ”´ Los insights son INVÃLIDOS si:
1. **Sin estructura** â†’ El resultado es un string genÃ©rico sin metadata Ãºtil
2. **Type desconocido** â†’ type = "unknown" (no pudo detectar quÃ© tipo de datos es)
3. **Metadata vacÃ­a** â†’ Tiene type definido pero TODAS las demÃ¡s keys estÃ¡n vacÃ­as/null/missing
4. **Error de ejecuciÃ³n** â†’ Contiene key "error" indicando que el cÃ³digo fallÃ³
5. **Sin valor** â†’ Los insights no aportan informaciÃ³n Ãºtil para resolver la tarea

ğŸŸ¢ Los insights son VÃLIDOS si:
1. **Metadata estructurada** â†’ Contiene informaciÃ³n organizada (no solo un string)
2. **Type identificado** â†’ DetectÃ³ el tipo de datos (pdf, image, email, etc.)
3. **Keys Ãºtiles** â†’ Tiene metadata relevante aunque sea parcial (ej: pages, format, size, etc.)
4. **Sin errores reales** â†’ No hay crashes ni fallos de ejecuciÃ³n
5. **Ayuda a la tarea** â†’ La informaciÃ³n es Ãºtil para el siguiente paso del workflow

âš ï¸ CASOS ESPECIALES:
- Si type="pdf" con has_text_layer=false â†’ ES VÃLIDO (indica que necesita OCR)
- Si type="image" con has_text=false â†’ ES VÃLIDO (indica que no tiene texto visible)
- Si type="email" con attachment_count=0 â†’ ES VÃLIDO (indica que no hay attachments)
- Metadata parcial es VÃLIDA si es Ãºtil (no necesita tener TODAS las keys posibles)

**IMPORTANTE:**
- SÃ© CRÃTICO: Si los insights no ayudan a resolver la tarea, mÃ¡rcalos como invÃ¡lidos
- Compara la TAREA con los INSIGHTS (Â¿sirven para resolverla?)
- Metadata vacÃ­a/genÃ©rica sin estructura â†’ INVÃLIDO
- Metadata estructurada aunque sea parcial â†’ VÃLIDO
- Distingue "cÃ³digo fallÃ³" (crash) vs "cÃ³digo funcionÃ³ pero detectÃ³ que no hay datos"

**Tu reason debe explicar**:
- Â¿QuÃ© tipo de informaciÃ³n se esperaba segÃºn la tarea?
- Â¿QuÃ© se obtuvo realmente en los insights?
- Â¿Por quÃ© es vÃ¡lido/invÃ¡lido?
- Si es invÃ¡lido: Â¿QuÃ© deberÃ­a mejorarse en el anÃ¡lisis? (insight para retry)
"""
        return prompt
